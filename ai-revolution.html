<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AI Revolution: From Perceptron to Transformer</title>
  <link rel="icon" type="image/png" href="/icon.png" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet" />
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background: #f9f9f9;
      margin: 0;
      color: #333;
    }
    header {
      background: #0f172a;
      color: white;
      padding: 2rem 1rem;
      text-align: center;
      position: relative;
    }
    .language-select {
      position: absolute;
      top: 1rem;
      right: 1rem;
    }
    select {
      padding: 0.25rem;
      font-size: 1rem;
      border-radius: 0.25rem;
    }
    main {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem 1rem;
    }
    h1 {
      font-size: 2rem;
    }
    p {
      line-height: 1.6;
      margin-bottom: 1rem;
    }
    .nav {
      margin-top: 3rem;
      display: flex;
      justify-content: space-between;
    }
    a {
      color: #2563eb;
      text-decoration: none;
      font-weight: 600;
    }
    footer {
      text-align: center;
      padding: 1rem;
      font-size: 0.9rem;
      color: #64748b;
    }
  </style>
</head>
<body>
  <header>
    <div class="language-select">
      <select id="languageSwitcher">
        <option value="en">English</option>
        <option value="id">Bahasa Indonesia</option>
      </select>
    </div>
    <h1 data-en="AI Revolution: From Perceptron to Transformer" data-id="Revolusi AI: Dari Perceptron ke Transformer">AI Revolution: From Perceptron to Transformer</h1>
  </header>
  <main>
    <p data-en="Feedforward in a perceptron is the process where inputs are multiplied by weights, summed, and passed through an activation function to produce an output. Backpropagation is the learning mechanism that adjusts these weights by calculating the error between predicted and actual values, then propagating this error backward using gradients." 
     data-id="Feedforward dalam perceptron adalah proses di mana input dikalikan dengan bobot, dijumlahkan, lalu dilewatkan melalui fungsi aktivasi untuk menghasilkan output. Backpropagation adalah mekanisme pembelajaran yang menyesuaikan bobot ini dengan menghitung kesalahan antara nilai prediksi dan nilai aktual, lalu menyebarkan kesalahan tersebut ke belakang menggunakan gradien.">
    Feedforward in a perceptron is the process where inputs are multiplied by weights, summed, and passed through an activation function to produce an output. Backpropagation is the learning mechanism that adjusts these weights by calculating the error between predicted and actual values, then propagating this error backward using gradients.
  </p>

  <p data-en="A single-layer perceptron can only solve linearly separable problems. It fails with non-linear problems like XOR because it cannot create complex decision boundaries. This limitation led to the development of Multi-Layer Perceptrons (MLPs), which introduce hidden layers and non-linear activation functions to model complex relationships." 
     data-id="Perceptron lapisan tunggal hanya dapat menyelesaikan masalah yang dapat dipisahkan secara linier. Ia gagal pada masalah non-linear seperti XOR karena tidak dapat membentuk batas keputusan yang kompleks. Keterbatasan ini mendorong pengembangan Multi-Layer Perceptrons (MLP), yang memperkenalkan lapisan tersembunyi dan fungsi aktivasi non-linear untuk memodelkan hubungan yang lebih kompleks.">
    A single-layer perceptron can only solve linearly separable problems. It fails with non-linear problems like XOR because it cannot create complex decision boundaries. This limitation led to the development of Multi-Layer Perceptrons (MLPs), which introduce hidden layers and non-linear activation functions to model complex relationships.
  </p>

  <p data-en="Recurrent Neural Networks (RNNs) are designed to handle sequential data like text, audio, or time series. They maintain a memory of previous inputs through loops within the network, allowing them to process data with temporal dependencies. However, they suffer from vanishing gradients in long sequences." 
     data-id="Recurrent Neural Network (RNN) dirancang untuk menangani data berurutan seperti teks, audio, atau deret waktu. RNN mempertahankan memori dari input sebelumnya melalui loop di dalam jaringan, memungkinkan pemrosesan data dengan ketergantungan waktu. Namun, RNN mengalami masalah vanishing gradient pada urutan yang panjang.">
    Recurrent Neural Networks (RNNs) are designed to handle sequential data like text, audio, or time series. They maintain a memory of previous inputs through loops within the network, allowing them to process data with temporal dependencies. However, they suffer from vanishing gradients in long sequences.
  </p>

  <p data-en="Long Short-Term Memory (LSTM) networks improve RNNs by introducing gates that control the flow of information, allowing them to remember important data for longer periods. Despite this, LSTMs are still sequential in nature and difficult to parallelize, making them inefficient on large datasets." 
     data-id="Long Short-Term Memory (LSTM) meningkatkan RNN dengan memperkenalkan gerbang (gates) yang mengatur aliran informasi, memungkinkan jaringan mengingat data penting dalam jangka waktu yang lebih lama. Meskipun demikian, LSTM masih bersifat sekuensial dan sulit untuk diparalelkan, sehingga kurang efisien untuk dataset besar.">
    Long Short-Term Memory (LSTM) networks improve RNNs by introducing gates that control the flow of information, allowing them to remember important data for longer periods. Despite this, LSTMs are still sequential in nature and difficult to parallelize, making them inefficient on large datasets.
  </p>

  <p data-en="Transformers revolutionized deep learning by eliminating recurrence and using self-attention mechanisms to process entire sequences in parallel. This allows them to scale efficiently and learn relationships between distant words or elements in a sequence, leading to breakthroughs in models like BERT and GPT." 
     data-id="Transformer merevolusi pembelajaran mendalam dengan menghilangkan rekursi dan menggunakan mekanisme self-attention untuk memproses seluruh urutan secara paralel. Ini memungkinkan model untuk diskalakan secara efisien dan mempelajari hubungan antara kata atau elemen yang berjauhan dalam sebuah urutan, menghasilkan terobosan dalam model seperti BERT dan GPT.">
    Transformers revolutionized deep learning by eliminating recurrence and using self-attention mechanisms to process entire sequences in parallel. This allows them to scale efficiently and learn relationships between distant words or elements in a sequence, leading to breakthroughs in models like BERT and GPT.
  </p>
    <div class="nav">
      <a href="ai-vs-traditional.html">&larr; Previous</a>
      <a href="transformer-overview.html">Next &rarr;</a>
    </div>
  </main>
  <footer>
    &copy; 2025 Seven Muslims. | <a href="/index.html">Home</a>
  </footer>
  <script>
    const switcher = document.getElementById('languageSwitcher');
    switcher.addEventListener('change', () => {
      const lang = switcher.value;
      document.querySelectorAll('[data-en]').forEach(el => {
        el.textContent = el.getAttribute(`data-${lang}`);
      });
    });
  </script>
</body>
</html>
